{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8c0ec9",
   "metadata": {},
   "source": [
    "# Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcae1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443\n"
     ]
    }
   ],
   "source": [
    "# single neuron\n",
    "\n",
    "weights = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias = 2\n",
    "\n",
    "output = sum([weights[i]*inputs[i] for i in range(len(weights))])+bias # this is the output from that neuron\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c924fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443\n"
     ]
    }
   ],
   "source": [
    "weights = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias = 2\n",
    "\n",
    "output = weights[0]*inputs[0]+weights[1]*inputs[1]+weights[2]*inputs[2]+bias # this is the output from that neuron\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2ea04",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e84a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443 1.56 2.838 [2.443, 1.56, 2.838]\n"
     ]
    }
   ],
   "source": [
    "# assuming 3 neurons in the layer we will get 3 outputs from that layer one from each layer\n",
    "\n",
    "weights1 = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "weights2 = [0.024,0.05,0.02]\n",
    "weights3 = [0.05,0.07,0.024]\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias1 = 2\n",
    "bias2 = 1\n",
    "bias3 = 2\n",
    "\n",
    "\n",
    "output1 = weights1[0]*inputs[0]+weights1[1]*inputs[1]+weights1[2]*inputs[2]+bias1 # this is the output from the neuron1\n",
    "output2 = weights2[0]*inputs[0]+weights2[1]*inputs[1]+weights2[2]*inputs[2]+bias2 # output from neuron2\n",
    "output3 = weights3[0]*inputs[0]+weights3[1]*inputs[1]+weights3[2]*inputs[2]+bias3 # output from neuron3\n",
    "\n",
    "output = [weights1[0]*inputs[0]+weights1[1]*inputs[1]+weights1[2]*inputs[2]+bias1, weights2[0]*inputs[0]+weights2[1]*inputs[1]+weights2[2]*inputs[2]+bias2,\n",
    "        weights3[0]*inputs[0]+weights3[1]*inputs[1]+weights3[2]*inputs[2]+bias3]\n",
    "print(output1,output2,output3, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee29392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1310000000000002, 1.184, 2.262]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#method 2\n",
    "\n",
    "weights = [weights1,weights2,weights3]\n",
    "biases = [bias1,bias2,bias3]\n",
    "inputs  = [1,2,3]\n",
    "\n",
    "outputs = []\n",
    "count = 0 \n",
    "for wt,bias in zip(weights,biases):\n",
    "    neuron_output = 0 \n",
    "    for w,i in zip(wt,inputs):\n",
    "        neuron_output+= w*i\n",
    "        \n",
    "    neuron_output+=bias\n",
    "    count+=1\n",
    "    outputs.append(neuron_output)\n",
    "    \n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80ccc7",
   "metadata": {},
   "source": [
    "#  With Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e861a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3540a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here y = WX+B acts similar to the linear equation. W varies slope varies, B varies interection on the axes varies which is similar to offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1325b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.131, 1.184, 2.262])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.dot(weights,inputs)+biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129ffeb",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872f096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.238 1.333 2.127]\n",
      " [2.58  1.762 2.304]\n",
      " [2.922 2.191 2.481]\n",
      " [3.264 2.62  2.658]]\n"
     ]
    }
   ],
   "source": [
    "# Batch processing is used for faster learning of the model, generalizing the model\n",
    "# Batch should be in a certain limit. If we take entire dataset a Batch it might lead to overfitting of the model\n",
    "\n",
    "weights = [weights1,weights2,weights3]\n",
    "biases = [bias1,bias2,bias3]\n",
    "\n",
    "#lets take input in batches we have 3 batches of inputs\n",
    "\n",
    "inputs = [[1,2,3],[4,5,6],[7,8,9],[10,11,12]]\n",
    "\n",
    "outputs = np.dot(inputs,weights)+biases\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8957651",
   "metadata": {},
   "source": [
    "# Adding one more layer with 4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7411353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1385  0.4326  1.73732 2.533  ]\n",
      " [1.3116  0.4098  2.05154 2.9398 ]\n",
      " [1.4847  0.387   2.36576 3.3466 ]\n",
      " [1.6578  0.3642  2.67998 3.7534 ]]\n"
     ]
    }
   ],
   "source": [
    "weights_2 = [[0.1,0.2,0.3],[-0.2,-0.1,0.5],[0.6,0.25,0.01],[0.2,0.5,0.7]]\n",
    "biases_2 = [0.01,-0.05,0.04,-0.07]\n",
    "\n",
    "# lets take input from previous layer as an input tho this layer\n",
    "outputs = np.dot(outputs,np.array(weights_2).transpose())+biases_2\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b35df2",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e38db0",
   "metadata": {},
   "source": [
    "\n",
    "Activation functions take the weighted sum of inputs and biases as an input and create an output\n",
    "\n",
    "Why Activation functions are required ? To get the granular output. Easy to calculate the losses \n",
    "\n",
    "Before Activation functions there were threshold functions are step functions. These give the output 1 if x>0 else 0\n",
    "\n",
    "There are no continuity at 0. When backpropagation introduced, with parital derivatives being used to calculate the loss, it will become tough to calculate the loss at 0\n",
    "\n",
    "Also giving 1 or 0 as an output will be merely classification of two classes. \n",
    "\n",
    "So Activation functions are introduced inplace of threshold functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8f816",
   "metadata": {},
   "source": [
    "We have multiple activation functions\n",
    "\n",
    "1. Sigmoid\n",
    "2. ReLu\n",
    "3. Tanh etc.\n",
    "\n",
    "We will start with ReLU as it is easy to perform, fast and performs good\n",
    "\n",
    "Relu : f(x) = x if x>0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24b7d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0.5, 0.07, 0, 0, 3]\n",
      "[0.   2.   0.5  0.07 0.   0.   3.  ]\n"
     ]
    }
   ],
   "source": [
    "# ReLU Activation function\n",
    "\n",
    "# using list \n",
    "\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "\n",
    "for i,x in enumerate(test_array):\n",
    "    if x<0:\n",
    "        test_array[i] = 0 \n",
    "\n",
    "print(test_array)\n",
    "        \n",
    "    \n",
    "# using numpy\n",
    "import numpy as np\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "x = np.maximum(0,test_array)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094c03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011434334402785674, 0.22966474583922664, 0.05124513151955508, 0.03333542411376307, 0.021902934401134184, 0.028123924471100584, 0.6242935052524349]\n",
      "1.0\n",
      "exponents >>  [[0.36787944 7.3890561  1.64872127]\n",
      " [1.07250818 0.70468809 0.90483742]]\n",
      "sums>>  [[9.40565681]\n",
      " [2.68203369]]\n",
      "norms >>  [[0.03911257 0.78559703 0.17529039]\n",
      " [0.39988617 0.26274394 0.33736989]]\n",
      "total_probabilities >  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Softmax activation function  (e^xi/sum(e^xi))\n",
    "\"\"\"\n",
    "If we use the relu or anyother function the values can be random, will become tough to calculate losses\n",
    "\n",
    "so exponential function is introduced which is softmax\n",
    "\n",
    "it is defined by calculating exponent of the values and then divide each value by sum of all values\n",
    "\n",
    "this is called normalization. It helps in calculating probabilities of various classes\n",
    "\n",
    "So softmax is exponentition followed by normalization\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "exponents = [math.e**x for x in test_array]\n",
    "total = sum(exponents)\n",
    "norms = [x/total for x in exponents]\n",
    "print(norms)\n",
    "print(sum(norms))\n",
    "\n",
    "\n",
    "\n",
    "#using numpy\n",
    "import numpy as np\n",
    "test_array = [[-1,2,0.5],[0.07,-0.35,-0.1]] # two batches of inputs \n",
    "exponents_ = np.exp(test_array) \n",
    "print(\"exponents >> \",exponents_)\n",
    "sums  = np.sum(np.exp(test_array),axis=1,keepdims=True) #axis 0: columns, axis 1: rows\n",
    "print(\"sums>> \",sums)\n",
    "norms = np.exp(test_array)/np.sum(np.exp(test_array),axis=1,keepdims=True)\n",
    "print(\"norms >> \", norms)\n",
    "total_probabilites = np.sum(norms,axis=1)\n",
    "print(\"total_probabilities > \", total_probabilites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036da0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8fa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e23ff1d",
   "metadata": {},
   "source": [
    "# Calculating Loss of a function\n",
    "\n",
    "Here as we are using the multiclass classification we will use the \"CATEGORICAL CROSS ENTROPY\" to calculate the loss\n",
    "\n",
    "Formula for the same is L = -ΣYi_true*log(Yi_pred)\n",
    "\n",
    "Why this : \n",
    "1. It is efficient\n",
    "2. Easy to calculate the loss and store values for backpropagation\n",
    "\n",
    "when you use \"One Hot Encoding\" it will reduces to Li = -log(Yi_pred)\n",
    "\n",
    "\"\"\"\n",
    "One Hot Encoding:\n",
    "\n",
    "if there are 3 classes [0,1,2]\n",
    "\n",
    "Yi_true for class1 (0) = [1,0,0]\n",
    "\n",
    "Yi_true for class2 (1) = [0,1,0]\n",
    "\n",
    "Yi_true for class3 (2) = [0,0,1]\n",
    "\n",
    "if you calculate the loss for class1 using the above function your loss will be L1 = -log(Y1_pred)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3f06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss >>  0.6931471805599453\n",
      "loss1 >>  0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# suppose your softmax outputs are = [0.5,0.251,0.249]\n",
    "\n",
    "ypred = [0.5,0.251,0.249]\n",
    "\n",
    "# you have to find out the loss \n",
    "\n",
    "# lets suppose your  values are [0,1,2] ==> [[1,0,0],[0,1,0],[0,0,2]]\n",
    "\n",
    "import math\n",
    "\n",
    "ytrue = [1,0,0] # one hot encoding\n",
    "\n",
    "loss = -(math.log(ypred[0])*ytrue[0] + \n",
    "         math.log(ypred[1])*ytrue[1] +\n",
    "         math.log(ypred[2])*ytrue[2])\n",
    "\n",
    "print(\"loss >> \", loss) \n",
    "\n",
    "loss1 = -(math.log(ypred[0])*ytrue[0]) # because of one hot encoding \n",
    "\n",
    "print(\"loss1 >> \", loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e65db480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss >>  [0.22314355 0.5798185  0.51082562]\n",
      "[[8.0e-01 1.0e-10 2.0e-01]\n",
      " [3.0e-01 5.6e-01 1.4e-01]\n",
      " [1.2e-01 2.8e-01 6.0e-01]]\n",
      "Loss >>  [0.22314355 0.5798185  0.51082562]\n"
     ]
    }
   ],
   "source": [
    "# but in general we use the batch of samples for the training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ypreds = [[0.8,0.01,0.19],[0.3,0.56,0.14],[0.12,0.28,0.60]] # 3 batches of outputs with probabilities\n",
    "\n",
    "ytrue = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "# now calculate the losses\n",
    "\n",
    "Loss = -np.sum(np.log(ypreds)*np.array(ytrue),axis=1)\n",
    "\n",
    "print(\"Loss >> \", Loss)\n",
    "# print(\"Loss for each class\",np.sum(Loss,axis=1))\n",
    "\n",
    "\"\"\"\n",
    "sometimes probabilites can be zero in that case we might get the error\n",
    "\n",
    "to handle that we have to clip the ypred values\n",
    "\"\"\"\n",
    "ypreds = [[0.8,0.0,0.2],[0.3,0.56,0.14],[0.12,0.28,0.60]] # 3 batches of outputs with probabilities\n",
    "\n",
    "yclipped = np.clip(ypreds,1e-10,1-1e-10) # handles the case of values that are zero\n",
    "\n",
    "print(yclipped)\n",
    "\n",
    "# now calculate the losses\n",
    "\n",
    "Loss = -np.sum(np.log(yclipped)*np.array(ytrue),axis=1)\n",
    "\n",
    "print(\"Loss >> \", Loss)\n",
    "# print(\"Loss for each class\",np.sum(Loss,axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b363278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 0\n",
      "0.56 1\n",
      "0.6 2\n",
      "[[8.0e-01 1.0e-10 2.0e-01]\n",
      " [3.0e-01 5.6e-01 1.4e-01]\n",
      " [1.2e-01 2.8e-01 6.0e-01]]\n",
      "[0.8  0.56 0.6 ]\n",
      "[0.22314355 0.5798185  0.51082562]\n"
     ]
    }
   ],
   "source": [
    "#if the values are given are not in the one hot encoding and rather normal list values like [0,1,2]\n",
    "\n",
    "# we have to take the corresponding values from the outputs \n",
    "ytrue =[0,1,2]\n",
    "\n",
    "for i,j in zip(yclipped,ytrue):\n",
    "    print(i[j],j)\n",
    "    \n",
    "x = np.array(yclipped)\n",
    "print(x)\n",
    "print(x[range(len(x)),ytrue])\n",
    "print(-np.log(x[range(len(x)),ytrue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb41c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
