{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8c0ec9",
   "metadata": {},
   "source": [
    "# Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcae1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443\n"
     ]
    }
   ],
   "source": [
    "# single neuron\n",
    "\n",
    "weights = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias = 2\n",
    "\n",
    "output = sum([weights[i]*inputs[i] for i in range(len(weights))])+bias # this is the output from that neuron\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c924fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443\n"
     ]
    }
   ],
   "source": [
    "weights = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias = 2\n",
    "\n",
    "output = weights[0]*inputs[0]+weights[1]*inputs[1]+weights[2]*inputs[2]+bias # this is the output from that neuron\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2ea04",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e84a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.443 1.56 2.838 [2.443, 1.56, 2.838]\n"
     ]
    }
   ],
   "source": [
    "# assuming 3 neurons in the layer we will get 3 outputs from that layer one from each layer\n",
    "\n",
    "weights1 = [0.04,0.023,0.015] # assuming 3 inputs are coming to the neuron\n",
    "weights2 = [0.024,0.05,0.02]\n",
    "weights3 = [0.05,0.07,0.024]\n",
    "inputs  = [5,6,7] # these are the inputs to that neuron\n",
    "bias1 = 2\n",
    "bias2 = 1\n",
    "bias3 = 2\n",
    "\n",
    "\n",
    "output1 = weights1[0]*inputs[0]+weights1[1]*inputs[1]+weights1[2]*inputs[2]+bias1 # this is the output from the neuron1\n",
    "output2 = weights2[0]*inputs[0]+weights2[1]*inputs[1]+weights2[2]*inputs[2]+bias2 # output from neuron2\n",
    "output3 = weights3[0]*inputs[0]+weights3[1]*inputs[1]+weights3[2]*inputs[2]+bias3 # output from neuron3\n",
    "\n",
    "output = [weights1[0]*inputs[0]+weights1[1]*inputs[1]+weights1[2]*inputs[2]+bias1, weights2[0]*inputs[0]+weights2[1]*inputs[1]+weights2[2]*inputs[2]+bias2,\n",
    "        weights3[0]*inputs[0]+weights3[1]*inputs[1]+weights3[2]*inputs[2]+bias3]\n",
    "print(output1,output2,output3, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee29392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1310000000000002, 1.184, 2.262]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#method 2\n",
    "\n",
    "weights = [weights1,weights2,weights3]\n",
    "biases = [bias1,bias2,bias3]\n",
    "inputs  = [1,2,3]\n",
    "\n",
    "outputs = []\n",
    "count = 0 \n",
    "for wt,bias in zip(weights,biases):\n",
    "    neuron_output = 0 \n",
    "    for w,i in zip(wt,inputs):\n",
    "        neuron_output+= w*i\n",
    "        \n",
    "    neuron_output+=bias\n",
    "    count+=1\n",
    "    outputs.append(neuron_output)\n",
    "    \n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80ccc7",
   "metadata": {},
   "source": [
    "#  With Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e861a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3540a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here y = WX+B acts similar to the linear equation. W varies slope varies, B varies interection on the axes varies which is similar to offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1325b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.131, 1.184, 2.262])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.dot(weights,inputs)+biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129ffeb",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872f096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.238 1.333 2.127]\n",
      " [2.58  1.762 2.304]\n",
      " [2.922 2.191 2.481]\n",
      " [3.264 2.62  2.658]]\n"
     ]
    }
   ],
   "source": [
    "# Batch processing is used for faster learning of the model, generalizing the model\n",
    "# Batch should be in a certain limit. If we take entire dataset a Batch it might lead to overfitting of the model\n",
    "\n",
    "weights = [weights1,weights2,weights3]\n",
    "biases = [bias1,bias2,bias3]\n",
    "\n",
    "#lets take input in batches we have 3 batches of inputs\n",
    "\n",
    "inputs = [[1,2,3],[4,5,6],[7,8,9],[10,11,12]]\n",
    "\n",
    "outputs = np.dot(inputs,weights)+biases\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8957651",
   "metadata": {},
   "source": [
    "# Adding one more layer with 4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7411353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1385  0.4326  1.73732 2.533  ]\n",
      " [1.3116  0.4098  2.05154 2.9398 ]\n",
      " [1.4847  0.387   2.36576 3.3466 ]\n",
      " [1.6578  0.3642  2.67998 3.7534 ]]\n"
     ]
    }
   ],
   "source": [
    "weights_2 = [[0.1,0.2,0.3],[-0.2,-0.1,0.5],[0.6,0.25,0.01],[0.2,0.5,0.7]]\n",
    "biases_2 = [0.01,-0.05,0.04,-0.07]\n",
    "\n",
    "# lets take input from previous layer as an input tho this layer\n",
    "outputs = np.dot(outputs,np.array(weights_2).transpose())+biases_2\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b35df2",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e38db0",
   "metadata": {},
   "source": [
    "\n",
    "Activation functions take the weighted sum of inputs and biases as an input and create an output\n",
    "\n",
    "Why Activation functions are required ? To get the granular output. Easy to calculate the losses \n",
    "\n",
    "Before Activation functions there were threshold functions are step functions. These give the output 1 if x>0 else 0\n",
    "\n",
    "There are no continuity at 0. When backpropagation introduced, with parital derivatives being used to calculate the loss, it will become tough to calculate the loss at 0\n",
    "\n",
    "Also giving 1 or 0 as an output will be merely classification of two classes. \n",
    "\n",
    "So Activation functions are introduced inplace of threshold functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8f816",
   "metadata": {},
   "source": [
    "We have multiple activation functions\n",
    "\n",
    "1. Sigmoid\n",
    "2. ReLu\n",
    "3. Tanh etc.\n",
    "\n",
    "We will start with ReLU as it is easy to perform, fast and performs good\n",
    "\n",
    "Relu : f(x) = x if x>0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24b7d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0.5, 0.07, 0, 0, 3]\n",
      "[0.   2.   0.5  0.07 0.   0.   3.  ]\n"
     ]
    }
   ],
   "source": [
    "# ReLU Activation function\n",
    "\n",
    "# using list \n",
    "\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "\n",
    "for i,x in enumerate(test_array):\n",
    "    if x<0:\n",
    "        test_array[i] = 0 \n",
    "\n",
    "print(test_array)\n",
    "        \n",
    "    \n",
    "# using numpy\n",
    "import numpy as np\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "x = np.maximum(0,test_array)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "094c03ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011434334402785674, 0.22966474583922664, 0.05124513151955508, 0.03333542411376307, 0.021902934401134184, 0.028123924471100584, 0.6242935052524349]\n",
      "1.0\n",
      "exponents >>  [[0.36787944 7.3890561  1.64872127]\n",
      " [1.07250818 0.70468809 0.90483742]]\n",
      "sums>>  [[9.40565681]\n",
      " [2.68203369]]\n",
      "norms >>  [[0.03911257 0.78559703 0.17529039]\n",
      " [0.39988617 0.26274394 0.33736989]]\n",
      "total_probabilities >  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Softmax activation function  (e^xi/sum(e^xi))\n",
    "\"\"\"\n",
    "If we use the relu or anyother function the values can be random, will become tough to calculate losses\n",
    "\n",
    "so exponential function is introduced which is softmax\n",
    "\n",
    "it is defined by calculating exponent of the values and then divide each value by sum of all values\n",
    "\n",
    "this is called normalization. It helps in calculating probabilities of various classes\n",
    "\n",
    "So softmax is exponentition followed by normalization\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "test_array = [-1,2,0.5,0.07,-0.35,-0.1,3]\n",
    "exponents = [math.e**x for x in test_array]\n",
    "total = sum(exponents)\n",
    "norms = [x/total for x in exponents]\n",
    "print(norms)\n",
    "print(sum(norms))\n",
    "\n",
    "\n",
    "\n",
    "#using numpy\n",
    "import numpy as np\n",
    "test_array = [[-1,2,0.5],[0.07,-0.35,-0.1]] # two batches of inputs \n",
    "exponents_ = np.exp(test_array) \n",
    "print(\"exponents >> \",exponents_)\n",
    "sums  = np.sum(np.exp(test_array),axis=1,keepdims=True) #axis 0: columns, axis 1: rows\n",
    "print(\"sums>> \",sums)\n",
    "norms = np.exp(test_array)/np.sum(np.exp(test_array),axis=1,keepdims=True)\n",
    "print(\"norms >> \", norms)\n",
    "total_probabilites = np.sum(norms,axis=1)\n",
    "print(\"total_probabilities > \", total_probabilites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c1aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f06b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
